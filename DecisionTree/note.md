#### 决策树的构造

**优点**：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。
**缺点**：可能会产生过度匹配的问题。
**适用数据类型**：**数值型**和**标称型**。

在构造决策树时，需要解决的第一个问题就是：**当前数据集上哪个特征在划分数据分类时起决定性作用**。

创建分支的伪代码：
检测数据集中的每个子项是否属于同一分类：
&emsp;If so return 类标签；
&emsp;Else
&emsp;&emsp;寻找划分数据集的最好特征；
&emsp;&emsp;划分数据集；
&emsp;&emsp;创建分支节点；
&emsp;&emsp;&emsp;for 每个划分的子集；
&emsp;&emsp;&emsp;&emsp;调用函数createBranch并增加返回结果到分支节点中
&emsp;&emsp;return 分支节点

#### 决策树的一般流程

（1）收集数据：可以使用任何方法。
（2）准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。
（3）分析数据：可以使用任何方法，构造树完成之后，应该检查图形是否符合预期。
（4）训练算法：构造树的数据结构。
（5）测试算法：使用经验树计算错误率。
（6）使用算法：监督学习算法通用的使用方法。

#### 信息增益

划分数据集的大原则是：将无序的数据变得更加有序。有多种划分数据集的方法，每种方法都有各自的优缺点。组织杂乱无章数据的一种方法就是**使用信息论度量信息**。

在划分数据集之前和之后信息发生的变化称为**信息增益（information）**，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，**获得信息增益最高的特征就是最好的选择**。集合信息的度量方式称为**香农熵**或者简称为**熵（entropy）**，熵越高，代表数据越混乱。

熵定义为信息的期望值。如果待分类的事务可能划分在多个分类之中，则符号$x_i$的信息定义为：
$$I(x_i) = -log_2p(x_i)$$
其中，p(x_i)是选择该分类的概率。

为了计算熵，我们需要计算所有类别所有可能包含的信息期望值，通过下面的公式得到：
$$H=-\sum_{i=1}^np(x_i)log_2p(x_i)$$
其中n是分类的数目。

#### 划分数据集

决策树算法需要划分数据集，并且度量划分数据集的熵，然后判断当前是否做出了正确的划分。算法将对每个特征划分数据集的结果计算一次信息熵，然后判断按照哪个特征划分数据集是最好的划分方式。

#### 递归构建决策树

第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。

递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。**如果所有实例具有相同的分类，则得到一个叶子节点或者终止块**。任何到达叶子节点的数据必然属于叶子节点的分类。

如果数据集已经处理了所有属性，但是类标签依然不是唯一的，此时我们需要决定如何定义该叶子节点，在这种情况下，我们通常会采用多数表决的方法决定该叶子节点的分类。

#### 测试和存储分类器

##### 测试算法：使用决策树执行分类

依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点；最后将测试数据定义为叶子节点所属的类型。

##### 决策树的存储

决策树的存储是很耗时的任务，即使处理很小的数据集，如前面的样本数据，也要花费几秒的时间，如果数据集很大，将会耗费很多计算时间。然而用创建好的决策树解决分类问题，则可以很快完成。因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树。为了解决这个问题，需要使用Python模块pickle序列化对象。序列化的对象可以在磁盘上保存，并在需要的时候读取出来。**任何对象的可以执行序列化操作**，字典对象也不例外。

