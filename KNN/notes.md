###k近邻算法（KNN）采用测量不同特征值之间的距离的方法进行分类；

**优点**：精度高、对异常值不敏感、无数据输入假定；
**缺点**：计算复杂度高、空间复杂度高。
**适用数据范围**：数值型和标称型；

**工作原理**：存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们只选择样本数据集中前ｋ个最相似的数据，这就是ｋ近邻算法中ｋ的出处，通常ｋ是不大于20的整数。最后，选择k个最相似数据中出现次数最多的分类，作为新数据的分类。

**k-近邻算法的伪代码**：
对未知类别属性的数据集中的每个点依次执行以下操作：
(1) 计算已知类别数据集中的点与当前点之间的距离；
(2) 按照距离递增次序排序；
(3) 选取与当前点距离最小的k个点；
(4) 确定前k个点所在类别的出现频率；
(5) 返回前k个点出现频率最高的类别作为当前点的预测分类；

如果特征都是等权重的，则要进行归一化；

k决策树是k近邻算法的优化版，可以节省大量的计算开销

k-近邻算法是分类数据最简单最有效的算法，本章通过两个例子讲述了如何使用k-近邻算法构造分类器。k-近邻算法是基于实例的学习，使用算法时我们必须有接近实际数据的训练样本数据。k-近邻算法必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间。此外，由于必须对数据集中的每个数据计算距离值，实际使用时可能非常耗时。
k近邻算法的另一个缺陷是它无法给出任何数据的基础结构信息，因此我们也无法知晓平均实例样本和典型实例样本具有什么特征。